{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "98f8d4df-b8d2-4a3a-a4f6-31758739dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import gutenberg\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e1130f80-5169-4f8b-9949-f75ad11cd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to work out English plurals. \n",
    "#Takes a singular noun and generates a plural form, though it is not always correct.\n",
    " \t\n",
    "def plural(word):\n",
    "    if word.endswith('y'):\n",
    "        return word[:-1] + 'ies'\n",
    "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
    "        return word + 'es'\n",
    "    elif word.endswith('an'):\n",
    "        return word[:-2] + 'en'\n",
    "    else:\n",
    "        return word + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ec05201a-f949-491c-b6fc-6cce15b16361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: ['baby', 'box', 'Man', 'House', 'Book']\n",
      "Plural: babies\n",
      "\n",
      "noun: ['baby', 'box', 'Man', 'House', 'Book']\n",
      "Plural: boxes\n",
      "\n",
      "noun: ['baby', 'box', 'Man', 'House', 'Book']\n",
      "Plural: Men\n",
      "\n",
      "noun: ['baby', 'box', 'Man', 'House', 'Book']\n",
      "Plural: Houses\n",
      "\n",
      "noun: ['baby', 'box', 'Man', 'House', 'Book']\n",
      "Plural: Books\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test the function plural() with 5 different nouns, two of which won't be in correct plural form generated by this function.\n",
    "\n",
    "#Test subject\n",
    "test = [\"baby\", \"box\", \"Man\", \"House\", \"Book\"]\n",
    "\n",
    "#Test plural()\n",
    "for word in test:\n",
    "    print(\"noun:\", test)\n",
    "    print(\"Plural:\", plural(word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b9e19829-1e06-45e1-a28a-08fe2f29656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 11: with very little to distress or vex her.\n",
      "Line 12: \n",
      "Line 13: She was the youngest of the two daughters of a most affectionate,\n",
      "Line 14: indulgent father; and had, in consequence of her sister's marriage,\n",
      "Line 15: been mistress of his house from a very early period.  Her mother\n",
      "Line 16: had died too long ago for her to have more than an indistinct\n",
      "Line 17: remembrance of her caresses; and her place had been supplied\n",
      "Line 18: by an excellent woman as governess, who had fallen little short\n",
      "Line 19: of a mother in affection.\n",
      "Line 20: \n",
      "Line 21: Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "Line 22: less as a governess than a friend, very fond of both daughters,\n",
      "Line 23: but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "Line 24: of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "Line 25: office of governess, the mildness of her temper had hardly allowed\n",
      "Line 26: her to impose any restraint; and the shadow of authority being\n"
     ]
    }
   ],
   "source": [
    "#Pick of one the texts from gutenberg corpus and display the lines 10 through 25 from the text\n",
    "\n",
    "#Pulling gutenberg\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "#Split\n",
    "lines = sample.split('\\n')\n",
    "\n",
    "#lines 10 - 25\n",
    "for i in range(10, 26):\n",
    "    print(f\"Line {i + 1}:\", lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "62c4e220-d928-4376-9c28-aaa6471bd89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 tokens of lines 11–26: ['to', 'impose', 'any', 'restraint', 'and', 'the', 'shadow', 'of', 'authority', 'being']\n"
     ]
    }
   ],
   "source": [
    "#Obtain the list of words for the text you picked and display the last 10 words\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# extract lines 11–26\n",
    "emma_lines = gutenberg.raw('austen-emma.txt').split('\\n')\n",
    "section_text = '\\n'.join(emma_lines[10:26])\n",
    "\n",
    "# Had to use RegexpTokenizer because I was getting Punkt errors\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "section_tokens = tokenizer.tokenize(section_text)\n",
    "\n",
    "# display the last 10 tokens\n",
    "print(\"Last 10 tokens of lines 11–26:\", section_tokens[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "95ebaaec-0dac-4ea5-9ebc-fdd8f40f1911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A function that finds words in the input text that are not in the standard English vocabulary.\n",
    "# It computes the vocabulary of a text, then removes all items that occur in an existing wordlist, \n",
    "# leaving just the uncommon or mis-spelt words.\n",
    "\n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "\n",
    "#example use\n",
    "unusual_words(['hello','NLP','AI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ff974adf-1bed-4591-848a-dd571e0d6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unusual/mis-spelt words:\n",
      "['allowed', 'caresses', 'ceased', 'daughters', 'died', 'fond', 'sisters', 'supplied', 'years', 'youngest']\n"
     ]
    }
   ],
   "source": [
    "##Use the function above to find unusual/mis-spelt words in your text that you picked from Gutenberg corpus earlier. \n",
    "\n",
    "words = unusual_words(section_tokens)\n",
    "\n",
    "print(\"Unusual/mis-spelt words:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "379316a9-9c9a-46b3-9b7b-39f5e70e57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the above function to include lemmatization applied to the text \n",
    "\n",
    "#Using lemmatizer from Wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lem_unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words()) \n",
    "    \n",
    "    #try lemmatization for our text\n",
    "    lemmatized_vocab = set(lemmatizer.lemmatize(w) for w in text_vocab)\n",
    "    \n",
    "    #use lemmatized words from our text to find unusual words\n",
    "    unusual = lemmatized_vocab - english_vocab\n",
    "    \n",
    "    return sorted(unusual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d18d75ae-2c0a-4ced-b90f-100d705d6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized unusual/mis-spelled words from the Emma section:\n",
      "['allowed', 'ceased', 'died', 'fond', 'supplied', 'youngest']\n"
     ]
    }
   ],
   "source": [
    "##Use the modified function for your text you picked from the Gutenberg corpus\n",
    "\n",
    "# Use the lemmatized unusual words function on section_tokens (Emma lines 11–26)\n",
    "lemmatized_unusual = lem_unusual_words(section_tokens)\n",
    "\n",
    "# Print the result\n",
    "print(\"Lemmatized unusual/mis-spelled words from the Emma section:\")\n",
    "print(lemmatized_unusual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df025fba-9bb5-4699-a7dd-d2f1f799f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment on differences in output you observe. Explain why. \n",
    "#\n",
    "#in the lummatized form it removed some words because they were turned singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b84dbb69-28aa-4338-9f03-b45c3f68f245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813953488372093"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A function to compute what fraction of words in a text are not in the stopwords list:\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "#example use\n",
    "content_fraction(\"This is an example sentence with stopwords.\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b1aa3f5-7948-4f69-a22c-393400f2b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'my dog loves to eat his food', 'the dog eats a bone', 'the dog scratched the cat', 'man eats food', 'dog eats grass']\n",
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 10, 'my': 11, 'loves': 9, 'to': 14, 'eat': 4, 'his': 8, 'food': 6, 'the': 13, 'eats': 5, 'bone': 1, 'scratched': 12, 'cat': 2, 'grass': 7}\n",
      "BoW representation for 'dog bites man':  [[1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]]\n",
      "BoW representation for 'my dog loves to eat his food.:  [[0 0 0 1 1 0 1 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Here we calculate bag of words representation. We use CountVectorizer from sklearn.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Our text is preprocessed to remove period.\n",
    "documents = [\"Dog bites man.\", \n",
    "             \"My dog loves to eat his food.\", \n",
    "             \"The dog eats a bone.\", \n",
    "             \"The dog scratched the cat\", \n",
    "             \"Man eats food.\", \n",
    "             \"Dog eats grass.\", ] \n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#Example use: see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'my dog loves to eat his food.: \",bow_rep[1].toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36106876-b3bb-4606-8583-07cf2690a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usaas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use the function, content_fraction, to find the fraction of words in your text \n",
    "#(the one you picked earlier from the Gutenberg corpus)\n",
    "#are not in the stopwords list and comment on the result\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Uses above function\n",
    "fraction = content_fraction(section_tokens)\n",
    "\n",
    "print(\"Fraction:\", round(fraction, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b710dcc-bf3f-418a-81da-7ef6858bed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 11-24 my output is .44 showing 44 percent is meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80c9658b-7cbb-47d6-8ec8-e7202e247cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we calculate bag of words representation. We use CountVectorizer from sklearn.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Our text is preprocessed to remove period.\n",
    "documents = [\"Dog bites man.\", \n",
    "             \"My dog loves to eat his food.\", \n",
    "             \"The dog eats a bone.\", \n",
    "             \"The dog scratched the cat\", \n",
    "             \"Man eats food.\", \n",
    "             \"Dog eats grass.\", ] \n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#Example use: see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'my dog loves to eat his food.: \",bow_rep[1].toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "15adc50b-2eee-4466-ac87-eb5c41b63d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: [[0 0 0 1 1 0 1 0 0 1 0 2 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## Get the BoW representation using this vocabulary, for a new text of your choice\n",
    "\n",
    "text = [\"My dog loves to eat my food.\"]\n",
    "\n",
    "#Transform using vectorize\n",
    "bow = count_vect.transform([text[0].lower().replace(\".\",\"\")])\n",
    "\n",
    "#Prints BoW vector\n",
    "print(\"BoW:\", bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51caf858-9dfa-4036-b222-09bee230aaa5",
   "metadata": {},
   "source": [
    "******************************************************\n",
    "## TDIDF\n",
    "\n",
    "In BoW, all the words in the text are treated equally important. There is no notion of some words in the document being more important than others. TF-IDF addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query.\n",
    "\n",
    "The next cell shows a simple example of how to get the TF-IDF representation of a document using sklearn's TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a44ac5ae-5f39-4adc-b517-7c75138d0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [2.25276297 2.25276297 2.25276297 1.15415068 2.25276297 1.55961579\n",
      " 1.84729786 2.25276297 2.25276297 2.25276297 1.84729786 2.25276297\n",
      " 2.25276297 1.84729786 2.25276297]\n",
      "----------\n",
      "All words in the vocabulary ['bites' 'bone' 'cat' 'dog' 'eat' 'eats' 'food' 'grass' 'his' 'loves'\n",
      " 'man' 'my' 'scratched' 'the' 'to']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.71890333 0.         0.         0.36831339 0.         0.\n",
      "  0.         0.         0.         0.         0.58951102 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.21030046 0.41048115 0.\n",
      "  0.33660042 0.         0.41048115 0.41048115 0.         0.41048115\n",
      "  0.         0.         0.41048115]\n",
      " [0.         0.64359624 0.         0.32973156 0.         0.44556967\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52775813 0.        ]\n",
      " [0.         0.         0.44936797 0.23022322 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44936797 0.73697633 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.51259296\n",
      "  0.60714432 0.         0.         0.         0.60714432 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.38819592 0.         0.52457317\n",
      "  0.         0.75771163 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a4b597b-e0f3-41cf-a1f8-002b500176fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF representation:\n",
      " [[0.         0.         0.         0.18187635 0.3550007  0.\n",
      "  0.29110565 0.         0.         0.3550007  0.         0.71000141\n",
      "  0.         0.         0.3550007 ]]\n"
     ]
    }
   ],
   "source": [
    "#Get the TFIDF representation using this vocabulary, for the text of your choice from earlier\n",
    "\n",
    "#New sentence\n",
    "text = [\"My dog loves to eat my sisters food.\"]\n",
    "\n",
    "#TFIDF vectorizer\n",
    "tfidf = tfidf.transform([text[0].lower().replace(\".\", \"\")])\n",
    "\n",
    "#TFIDF vector\n",
    "print(\"TFIDF representation:\\n\", tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9e24a-ca8d-4abc-b7b9-1360db018cce",
   "metadata": {},
   "source": [
    "**************************************************************\n",
    "\n",
    "### Precision, Recall, F1\n",
    "\n",
    "A Named Entity Recognition (NER) system is evaluated on a dataset with three entity types: PERSON, ORG, and LOC. The confusion matrix below shows how the system performed:\n",
    "Calculate the Precision, Recall, and F1 Score for the PERSON class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64cefa-0bf3-4e38-872d-220dffe6ae5b",
   "metadata": {},
   "source": [
    "|                    | Predicted: PERSON | Predicted: ORG | Predicted: LOC |\n",
    "| ------------------ | ----------------- | -------------- | -------------- |\n",
    "| **Actual: PERSON** | 45                | 5              | 3              |\n",
    "| **Actual: ORG**    | 4                 | 45             | 7              |\n",
    "| **Actual: LOC**    | 3                 | 3              | 48             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e4da904-f901-448f-acc0-323d22da48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for PERSON: 0.865\n",
      "Recall for PERSON: 0.849\n",
      "F1 Score for PERSON: 0.857\n"
     ]
    }
   ],
   "source": [
    "#Calculate the Precision, Recall, and F1 Score for the PERSON class.\n",
    "#Show your work\n",
    "\n",
    "#TP = 45, FP = 7, Gn = 8\n",
    "\n",
    "#Precision = TP / TP + FP so 45 / 45+7 = 0.865\n",
    "#Recall = TP / TP + FN so 45 / 45+8 = 0.849\n",
    "#F1 = 2 x precision x recall / precision + recall = 2 x 0.865 x 0.849 / 0.865 + 0.849 = 0.857\n",
    "\n",
    "\n",
    "TP = 45\n",
    "FP = 4 + 3\n",
    "FN = 5 + 3\n",
    "\n",
    "# Precision, Recall, F1 calculations\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "#Coded calculations\n",
    "print(\"Precision for PERSON:\", round(precision, 3))\n",
    "print(\"Recall for PERSON:\", round(recall, 3))\n",
    "print(\"F1 Score for PERSON:\", round(f1_score, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76206211-db1e-41ce-86f5-e74fc5471eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
